{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT Sentence Embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pqa-7WXBAw8q"
      },
      "source": [
        "# 1. Loading Pre-Trained BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RfUN_KolV-f",
        "outputId": "ebf5da6f-8622-48be-9621-2d39777b0367",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install pytorch-pretrained-bert --quiet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 123 kB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 132 kB 45.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 79 kB 2.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 8.7 MB 61.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 138 kB 53.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 60.1 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJEnBJ3gHTsQ",
        "outputId": "783e721c-6caf-4ca4-ee96-4e3b70e82dbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary-multilingual)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n",
            "100%|██████████| 213450/213450 [00:00<00:00, 2126395.54B/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gsyrAwYvBfC"
      },
      "source": [
        "## 2. Sentence Tokenization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WafgQPLAWmo"
      },
      "source": [
        "BERT provides its own tokenizer, which we imported above. Let's see how it handles the below sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg0P9rFxJwwp",
        "outputId": "640890ee-0139-4645-eac1-cd28224c6b47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "text = \"Agregar mucha salsa molcajeteada, limones, y totopos ... gracias \"\n",
        "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "# Tokenize our sentence with the BERT tokenizer.\n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "segments_ids = [1] * len(tokenized_text)\n",
        "\n",
        "# Map the token strings to their vocabulary indeces.\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "# Print out the tokens.\n",
        "print (tokenized_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'A', '##g', '##re', '##gar', 'much', '##a', 'sa', '##ls', '##a', 'm', '##ol', '##ca', '##jet', '##ead', '##a', ',', 'limo', '##nes', ',', 'y', 'to', '##top', '##os', '.', '.', '.', 'g', '##rac', '##ias', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-nY9LASLr2L"
      },
      "source": [
        "# 3. Extracting Embeddings \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_t4cM6KLc98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2988d858-6e25-4cfd-d7ec-8ff94672047a"
      },
      "source": [
        "# Convert inputs to PyTorch tensors\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "# Load pre-trained model (weights)\n",
        "model = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 404400730/404400730 [00:11<00:00, 36367788.89B/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): BertLayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En4JZ41fh6CI",
        "outputId": "eab7c0c2-0938-4657-c29e-daecb68b8f9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Predict hidden states features for each layer\n",
        "with torch.no_grad():\n",
        "    encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
        "\n",
        "# Concatenate the tensors for all layers. We use `stack` here to\n",
        "# create a new dimension in the tensor.\n",
        "token_embeddings = torch.stack(encoded_layers, dim=0)\n",
        "\n",
        "# Remove dimension 1, the \"batches\".\n",
        "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "\n",
        "# Swap dimensions 0 and 1.\n",
        "token_embeddings = token_embeddings.permute(1,0,2)\n",
        "\n",
        "token_embeddings.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([31, 12, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76TdtFH8NM9q"
      },
      "source": [
        "## 3.1 Word Vectors\n",
        "\n",
        "There are many methods to extract the word vectors from BERT. A simple solution is to create the word vectors by summing together the last four layers.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv42h9jANMRf",
        "outputId": "58050f26-a713-4c82-ebb6-3014a6e19a29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Stores the token vectors, with shape [23 x 768]\n",
        "token_vecs_sum = []\n",
        "\n",
        "# `token_embeddings` is a [23 x 12 x 768] tensor.\n",
        "\n",
        "# For each token in the sentence...\n",
        "for token in token_embeddings:\n",
        "\n",
        "    # `token` is a [12 x 768] tensor\n",
        "\n",
        "    # Sum the vectors from the last four layers.\n",
        "    sum_vec = torch.sum(token[-4:], dim=0)\n",
        "    \n",
        "    # Use `sum_vec` to represent `token`.\n",
        "    token_vecs_sum.append(sum_vec)\n",
        "\n",
        "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape is: 31 x 768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHHr_Z0SBIc-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6102d35-4d1d-443c-d58a-c99a9412f205"
      },
      "source": [
        "token_vecs_sum[3][:15]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 4.9341, -5.9615, -1.0122,  4.8388,  1.4223, -0.4690, -0.3854,  2.5350,\n",
              "        -2.0847, -1.9974, -2.5810, -0.1155,  6.2884, -1.1851, -3.7856])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQaco6jRLkXn"
      },
      "source": [
        "## 3.2 Sentence Vectors\n",
        "\n",
        "\n",
        "To get a single vector for our entire sentence we have multiple application-dependent strategies, but a simple approach is to average the second to last hiden layer of each token producing a single 768 length vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zn0n2S-FWZih"
      },
      "source": [
        "# `encoded_layers` has shape [12 x 1 x 23 x 768]\n",
        "\n",
        "# `token_vecs` is a tensor with shape [23 x 768]\n",
        "token_vecs = encoded_layers[11][0]\n",
        "\n",
        "# Calculate the average of all 23 token vectors.\n",
        "sentence_embedding = torch.mean(token_vecs, dim=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQv0FL8VWadn",
        "outputId": "d5a886b3-c2a6-45ef-cb4d-854d965bf21d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print (\"Our final sentence embedding vector of shape:\", sentence_embedding.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our final sentence embedding vector of shape: torch.Size([768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRkpVuOFBjzn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbf78f0a-0fef-473d-b92d-675d164eb05d"
      },
      "source": [
        "sentence_embedding"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1.5676e-01,  9.0309e-02,  2.3519e-01,  2.7717e-01, -2.8722e-02,\n",
              "         2.5228e-01, -2.7609e-02,  1.5562e-01, -1.9589e-01,  4.9751e-02,\n",
              "        -1.1397e-02,  4.3979e-01, -2.2737e-01,  2.9419e-01, -1.5648e-02,\n",
              "        -1.0231e-01,  1.2727e-01,  3.0744e-01, -1.7944e-01,  1.5354e-01,\n",
              "         5.0796e-02, -9.8759e-02, -2.8601e-01,  7.6843e-02, -1.8348e-01,\n",
              "        -6.3703e-01, -9.1637e-02,  4.0376e-01, -2.4961e-01,  1.4805e-01,\n",
              "        -2.0682e-02,  5.5793e-03, -8.4749e-02,  2.7743e-02, -1.0231e-02,\n",
              "         2.0917e-01, -7.3548e-02,  2.4858e-01, -2.3741e-01, -3.5060e-02,\n",
              "         2.1387e-01, -3.2657e-02, -1.7652e-01,  4.3070e-01,  2.9573e-01,\n",
              "        -2.4952e-01,  5.1670e-02, -1.6528e-01, -5.4836e-01,  1.1053e-01,\n",
              "         1.8437e-01,  1.3851e-01, -1.0443e-02, -8.8564e-02,  4.7423e-01,\n",
              "        -1.6720e-01, -2.8161e-01, -2.8278e-01,  1.6190e-02,  6.1298e-02,\n",
              "        -2.6301e-01,  3.9287e-01,  2.6594e-01,  1.0703e-01,  1.5469e-01,\n",
              "         1.4666e-01,  1.2119e-01, -1.1021e-01,  2.7049e-01, -2.0445e-01,\n",
              "        -7.2827e-02, -1.2024e-01, -3.0657e-02, -1.3529e-01,  2.7245e-01,\n",
              "        -1.9886e-01,  2.3156e-01, -3.8552e-03, -4.5034e-03, -3.2848e-03,\n",
              "         1.9624e-01,  1.1436e-01,  4.9539e-02,  2.3832e-02, -1.2026e-01,\n",
              "         9.2324e-02,  4.6401e-02, -1.5338e-01,  2.1722e-01,  1.9442e-01,\n",
              "         1.4080e-01, -1.6914e-02,  3.3103e-01, -1.8447e-01,  7.0460e-02,\n",
              "         4.0532e-01, -3.4645e-01, -3.9223e-01, -1.8941e-03, -5.5769e-02,\n",
              "         9.5361e-02, -1.7981e-02, -2.8781e-01,  6.0757e-02,  9.3208e-02,\n",
              "        -1.3353e-01,  4.4488e-02, -3.4692e-01, -3.0394e-01,  1.6845e-01,\n",
              "         1.2389e-01, -2.3326e-01, -2.0068e-01, -4.7088e-01,  6.3973e-02,\n",
              "        -1.1865e-02, -6.8120e-02,  2.4796e-01,  4.0961e-02,  1.1528e-01,\n",
              "        -3.0584e-02, -5.6845e-02, -9.7287e-04,  3.2096e-01,  1.3587e-01,\n",
              "         9.2432e-02,  1.1300e-01,  5.7723e-02,  2.0179e-01, -7.1388e-02,\n",
              "        -1.4847e-01, -2.0185e-01,  5.2732e-01,  1.7858e-01,  1.1876e-02,\n",
              "         6.6864e-02, -5.2967e-02, -1.4562e-01, -8.2319e-01, -4.9354e-01,\n",
              "        -2.5415e-03, -2.1029e-01,  1.5126e-01, -2.7188e-02, -8.0954e-02,\n",
              "         7.0082e-02, -3.5136e-01, -1.3277e-01,  2.4004e-01, -1.2075e-01,\n",
              "        -1.2730e-01, -5.9818e-01,  3.4402e-02,  1.6010e-01,  1.6552e-02,\n",
              "        -1.3956e-01, -1.2046e-01,  3.8689e-02,  4.0405e-01, -3.4025e-01,\n",
              "        -6.9538e-03,  3.8803e-01,  2.4976e-02,  2.2449e-01, -4.5304e-02,\n",
              "         3.3521e-02,  2.7700e-01,  2.7351e-03, -4.5995e-01,  4.6072e-01,\n",
              "        -1.3990e-01,  3.4836e-02, -4.9024e-01,  5.6047e-01, -2.1101e-02,\n",
              "         3.2498e-01,  2.4239e-01, -1.5113e-01, -4.3609e-01,  1.7798e-01,\n",
              "         2.1426e-01, -1.1275e-01, -1.6023e-01, -6.3637e-01,  8.3338e-02,\n",
              "         3.0036e-01,  5.5896e-02, -3.7457e-01,  5.3616e-01,  2.6902e-01,\n",
              "         1.8213e-01, -1.9262e-01, -1.0485e-02, -1.9179e-01, -3.8257e-01,\n",
              "        -3.0331e-01,  2.6794e-01, -6.6121e-02,  2.7979e-01, -3.1598e-01,\n",
              "         2.2178e-01,  1.4917e-01, -2.8839e-01,  7.3577e-02,  3.1169e-01,\n",
              "         2.6827e-01, -3.5052e-01,  3.6715e-01, -3.8260e-01, -5.6303e-01,\n",
              "         3.7226e-02, -3.6328e-02,  1.1285e-01, -2.3378e-01,  2.6612e-02,\n",
              "        -2.3792e-01, -1.3770e-01, -1.6791e-01,  4.4176e-01,  1.3036e-01,\n",
              "        -1.1155e-01, -2.2136e-01,  1.6600e-01, -1.2965e-01, -1.3751e-01,\n",
              "        -5.1510e-03, -1.3615e-01, -1.6438e-01,  2.3666e-01,  1.0372e-02,\n",
              "         3.8355e-01,  3.7581e-01,  2.0372e-01, -4.4086e-01, -2.1512e-01,\n",
              "         1.3921e-01,  6.3353e-02,  8.3098e-02, -3.9427e-01,  2.1980e-01,\n",
              "         3.3776e-02, -1.2120e-01,  6.5413e-01,  1.2904e-01, -2.2145e-01,\n",
              "         4.9193e-01, -2.6376e-01,  4.9829e-01, -2.2790e-01,  4.1216e-01,\n",
              "        -1.9445e-01, -3.6624e-01,  5.6165e-01, -1.9106e-01,  7.7101e-01,\n",
              "         1.9475e-01,  3.1990e-02,  1.9169e-01, -3.3216e-01, -7.6845e-02,\n",
              "         3.8231e-02, -2.0781e-01,  1.5068e-01,  1.0734e-01,  1.8759e-01,\n",
              "        -2.3641e-01,  9.8338e-02,  1.2902e-01, -4.7320e-01,  5.1324e-02,\n",
              "         2.4535e-01,  3.1887e-01, -1.7891e-01,  2.7856e-01,  7.8983e-02,\n",
              "         3.3792e-01,  5.0150e-01, -4.3688e-01, -9.6349e-02, -6.8275e-02,\n",
              "         1.6512e-01,  2.3474e-01,  2.2532e-01, -1.8575e-01, -3.1046e-01,\n",
              "        -1.6289e-01, -1.5756e-01, -6.0471e-03, -1.3764e-01, -7.6813e-02,\n",
              "        -2.0722e-01,  2.5355e-01, -2.8576e-01, -3.1725e-01,  3.1537e-01,\n",
              "         2.9731e-01, -4.0469e-01,  2.3905e-01, -2.8645e-01,  2.1739e-01,\n",
              "         1.9836e-01, -1.6746e-01, -1.9713e-01,  2.9239e-01, -3.6854e-01,\n",
              "         1.6240e-01,  1.0027e-01,  8.6652e-02, -2.6228e-01,  1.0253e-01,\n",
              "         6.2895e-01,  7.9546e-02, -5.9928e-01,  2.9300e-01, -3.1614e-01,\n",
              "        -3.8018e-01, -3.3446e-01,  1.4784e-01, -1.8272e-01,  4.6298e-02,\n",
              "        -7.1736e-02,  3.3898e-01,  1.0273e-01, -1.0685e-01, -4.1400e-01,\n",
              "         1.8753e-02, -2.1366e-01, -9.9958e-02,  9.4127e-02, -9.9425e-02,\n",
              "         7.6413e-03, -8.1060e-02,  4.9449e-02, -2.5396e-01,  1.3088e-01,\n",
              "        -6.0705e-02, -3.3184e-01,  1.1097e-01, -4.5878e-01,  3.9175e-01,\n",
              "         1.3352e-01,  1.2199e-02,  5.3805e-02,  1.7289e-01,  1.4570e-01,\n",
              "         3.4887e-01,  1.5726e-02, -1.3667e-01,  1.9232e-02, -7.9142e-02,\n",
              "         1.7404e-01,  1.0745e-01, -1.8977e-01,  1.0837e-02, -4.4289e-02,\n",
              "         3.1643e-02, -1.3593e-01,  1.4071e-01,  2.4817e-01,  1.5338e-01,\n",
              "        -9.6664e-02, -3.1337e-01,  1.0756e-01,  1.7378e-01, -8.1325e-02,\n",
              "        -2.7816e-01, -2.1186e-01, -9.2081e-02, -3.5559e-01, -1.5779e-01,\n",
              "        -3.1640e-02, -2.7001e-01, -8.8447e-02,  2.0570e-01, -7.8722e-03,\n",
              "         2.6537e-01, -5.1532e-01,  1.3778e-01, -3.1079e-01, -2.1826e-01,\n",
              "        -2.1808e-01, -2.1302e-01, -9.1567e-02,  1.7409e-02,  9.3281e-02,\n",
              "         1.1568e-01,  4.3557e-01,  4.4121e-01,  1.6173e-02,  8.5567e-02,\n",
              "        -2.3086e-02,  2.1964e-01,  3.3389e-01, -3.1838e-01, -2.0247e-01,\n",
              "         8.6908e-02, -1.1952e-01,  2.6714e-01, -9.6712e-03,  3.8355e-02,\n",
              "        -6.7285e-02,  7.0458e-02, -4.0162e-01,  1.7021e-01,  1.0719e-01,\n",
              "        -1.8463e-01, -2.4593e-01,  1.2828e-01, -7.8668e-02,  5.9277e-02,\n",
              "         1.2514e-01, -5.2141e-01,  1.9521e-01,  2.9680e-01,  2.0141e-01,\n",
              "        -4.2439e-01,  7.9615e-02, -1.0399e-01,  2.4307e-01,  1.9270e-01,\n",
              "         1.7350e-01, -2.1626e-02,  8.5483e-02,  1.3283e-01,  7.5995e-02,\n",
              "        -1.4896e-01,  5.6485e-02, -6.5670e-02, -1.3539e-01, -8.6110e-02,\n",
              "         1.1658e-01, -1.4273e-01, -3.4514e-01,  3.7246e-01,  9.7439e-02,\n",
              "        -7.4148e-02, -2.1660e-01, -2.5929e-01, -2.5186e-02,  6.0736e-02,\n",
              "         1.3463e-01, -3.0705e-01, -1.4335e-01, -1.5739e-01,  2.7412e-01,\n",
              "        -6.5734e-02,  8.6861e-02, -6.6296e-03, -1.3186e-01,  1.3960e-01,\n",
              "         4.9491e-02, -1.6456e-01, -5.5161e-02,  6.5277e-02,  1.6937e-01,\n",
              "        -5.1939e-01,  1.2435e-01,  1.4691e-01,  2.3012e-01,  1.7388e-01,\n",
              "        -2.8309e-01,  3.2325e-01, -1.3710e-01,  1.2524e-01,  2.4304e-01,\n",
              "        -2.6327e-01, -1.0671e-01,  7.8577e-01, -7.6533e-02,  2.5800e-01,\n",
              "         2.4751e-01,  3.1426e-01,  1.7870e-01, -2.8140e-02, -1.6852e-01,\n",
              "         1.6901e-01,  1.9875e-01,  2.4257e-01, -9.4173e-03,  1.5071e-01,\n",
              "        -2.8919e-01,  1.5249e-01, -1.7954e-01,  8.8237e-02,  2.5362e-01,\n",
              "         3.3620e-01,  4.1926e-01, -8.8220e-02, -2.1273e-01,  2.1360e-02,\n",
              "         2.4710e-01, -1.8368e-02,  2.3283e-02,  5.4182e-02, -1.5587e-01,\n",
              "         3.7798e-02,  5.1332e-02, -3.9067e-01,  8.3042e-02, -1.1673e-01,\n",
              "        -1.1898e-01, -2.2272e-01,  3.6927e-02,  1.6912e-01, -9.6647e-02,\n",
              "         5.5249e-01, -1.3801e-01, -4.0355e-01, -1.2220e-02,  5.3098e-02,\n",
              "        -5.0987e-01, -8.0816e-01,  1.2093e-01, -5.2574e-02, -2.7690e-01,\n",
              "         8.8735e-02, -2.4193e-01,  9.7758e-02,  1.0393e-01,  1.3998e-01,\n",
              "         2.9537e-01, -1.8768e-01, -1.1803e-01, -2.4168e-01,  4.6989e-02,\n",
              "        -2.2445e-01,  5.7077e-01,  1.5087e-02,  1.0503e-01, -3.1319e-02,\n",
              "         6.9600e-02, -9.0017e-02,  2.7728e-01, -8.8998e-02,  2.2287e-01,\n",
              "        -9.1567e-02, -2.5188e-01, -2.0373e-01, -2.4184e-01,  2.0198e-01,\n",
              "         4.1378e-02,  4.3308e-01,  5.5232e-02, -2.4005e-01,  3.6826e-01,\n",
              "         5.3675e-01, -1.2580e-01,  1.1280e-01,  3.8933e-01,  1.6416e-03,\n",
              "        -1.4203e-01,  4.1253e-01, -1.0737e-01, -1.6547e-01, -3.7192e-02,\n",
              "         5.5512e-01, -5.5979e-03, -7.9009e+00,  2.1368e-01, -1.7928e-01,\n",
              "        -5.9418e-02,  2.2638e-01, -7.0066e-02,  1.9867e-01,  1.7893e-01,\n",
              "        -1.8014e-02, -6.2793e-02,  1.1404e-01, -1.5634e-01,  1.3372e-01,\n",
              "         3.1881e-01,  2.2712e-01, -4.3367e-01,  3.4627e-01,  2.2693e-01,\n",
              "         5.9085e-01, -1.5335e-01, -1.0440e-01, -1.4621e-01,  7.1004e-02,\n",
              "         5.2894e-02,  2.9271e-01, -6.9423e-02,  1.8441e-01, -6.5995e-02,\n",
              "         4.4364e-01,  1.7969e-01, -2.9549e-01,  3.1627e-01, -3.5783e-02,\n",
              "        -3.0866e-01, -3.3508e-02,  2.0910e-01,  3.1557e-01, -1.3080e-01,\n",
              "         3.1316e-02, -5.2574e-01, -4.2929e-02,  5.2158e-02, -6.1241e-03,\n",
              "        -1.3298e-01,  2.5302e-01, -1.6869e-01, -1.9518e-02,  1.0013e-03,\n",
              "        -4.2107e-02, -1.5382e-01,  3.6888e-01,  2.9968e-01,  9.1908e-02,\n",
              "        -3.6984e-01, -1.9091e-02,  5.3149e-02, -5.1945e-01,  3.9984e-01,\n",
              "         2.9662e-01,  6.9221e-02, -9.9624e-03, -3.3255e-02,  9.6482e-03,\n",
              "        -3.4931e-01, -8.3232e-02,  7.5828e-02, -3.2718e-01,  8.5375e-02,\n",
              "         9.6754e-02, -3.7934e-01, -4.1073e-01,  6.4564e-04, -9.6015e-02,\n",
              "        -2.5233e-01, -1.5689e-01, -2.1814e-01, -6.0836e-02,  4.2984e-02,\n",
              "         2.7151e-01,  1.5197e-01, -1.4185e-01, -7.9469e-02, -3.8324e-01,\n",
              "        -1.7140e-01, -1.1470e-01, -1.4135e-01, -3.9993e-01, -3.1042e-02,\n",
              "        -5.0340e-01, -6.4843e-02,  3.1633e-01, -1.7410e-01,  1.5043e-01,\n",
              "         2.4733e-01, -3.5868e-01,  1.1562e-01, -1.2798e-01, -3.4758e-01,\n",
              "        -9.5296e-02,  4.9847e-02, -1.3976e-01, -2.5480e-02, -3.3142e-02,\n",
              "         9.7614e-02,  2.4802e-01,  3.3990e-01,  2.4072e-02, -5.3582e-01,\n",
              "        -6.3072e-02,  9.7434e-02,  3.7115e-01,  6.0164e-01, -1.9747e-01,\n",
              "         1.1453e-01,  1.7993e-01,  4.7801e-01,  3.1747e-01, -7.1746e-02,\n",
              "        -3.0914e-02, -4.0231e-01,  3.2018e-02, -2.0468e-02,  5.1846e-01,\n",
              "        -8.0113e-02,  1.5567e-01,  3.0817e-01,  5.4830e-01,  7.5530e-02,\n",
              "         7.4676e-02, -7.6326e-02,  2.9158e-01,  5.5402e-01, -4.0575e-01,\n",
              "         4.6489e-01, -1.1677e-01, -1.7126e-01,  1.3401e-01, -1.9296e-01,\n",
              "        -3.4444e-01,  2.3245e-01, -5.0130e-01, -3.5834e-01,  2.1404e-01,\n",
              "         2.8171e-01,  3.7295e-01,  1.1626e-01,  8.9709e-02,  1.7283e-01,\n",
              "         7.8259e-02, -4.1044e-01, -1.2625e-01,  4.6419e-01,  1.2349e-01,\n",
              "        -3.6851e-01, -4.7941e-01, -1.5404e-01, -7.7226e-02,  2.5527e-01,\n",
              "        -1.6853e-01, -1.1724e-01, -5.0285e-02,  4.1464e-01,  1.0483e-01,\n",
              "         5.0534e-02,  1.4108e-01, -2.1294e-01,  1.5359e-01, -1.5143e-02,\n",
              "        -1.0748e-01, -1.7877e-01, -1.0514e-01,  1.6952e-02,  1.9744e-01,\n",
              "         9.1620e-02, -1.5705e-01, -3.0934e-01,  1.6035e-01, -4.1276e-02,\n",
              "        -3.6079e-01,  3.1110e-01, -7.8875e-02,  4.2299e-01,  3.4321e-01,\n",
              "        -2.9969e-01, -1.9867e-01,  4.0886e-01,  1.0143e-01,  3.6624e-01,\n",
              "         7.4180e-02, -1.9090e-01, -1.2714e-01, -3.2096e-01, -3.2822e-01,\n",
              "         2.1389e-01, -2.6752e-02,  2.6553e-01, -4.1649e-01,  4.2521e-02,\n",
              "        -1.1791e-01,  3.8554e-01, -7.5115e-02,  1.3972e-01,  1.1739e-01,\n",
              "        -5.3172e-02, -1.8361e-01, -1.4695e-01, -3.9158e-01,  9.5828e-02,\n",
              "         2.7648e-01, -1.7646e-01,  9.8800e-02])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT Embeddings for FakeNews Dataset Articles"
      ],
      "metadata": {
        "id": "nn6R088LKs-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv(\"./train.tsv\", sep=\"\\t\", header=None)\n",
        "data = train_df.values\n",
        "\n",
        "articles = [article[2] for article in data]\n",
        "\n",
        "article_embeddings = []\n",
        "for text in articles:\n",
        "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
        "    article_embeddings.append(sentence_embedding)\n",
        "\n",
        "print(article_embeddings[0])"
      ],
      "metadata": {
        "id": "e4lcAWWmKyyX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ccfceaf-d907-4c69-ff18-2bcf0fab67a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1.5676e-01,  9.0309e-02,  2.3519e-01,  2.7717e-01, -2.8722e-02,\n",
            "         2.5228e-01, -2.7609e-02,  1.5562e-01, -1.9589e-01,  4.9751e-02,\n",
            "        -1.1397e-02,  4.3979e-01, -2.2737e-01,  2.9419e-01, -1.5648e-02,\n",
            "        -1.0231e-01,  1.2727e-01,  3.0744e-01, -1.7944e-01,  1.5354e-01,\n",
            "         5.0796e-02, -9.8759e-02, -2.8601e-01,  7.6843e-02, -1.8348e-01,\n",
            "        -6.3703e-01, -9.1637e-02,  4.0376e-01, -2.4961e-01,  1.4805e-01,\n",
            "        -2.0682e-02,  5.5793e-03, -8.4749e-02,  2.7743e-02, -1.0231e-02,\n",
            "         2.0917e-01, -7.3548e-02,  2.4858e-01, -2.3741e-01, -3.5060e-02,\n",
            "         2.1387e-01, -3.2657e-02, -1.7652e-01,  4.3070e-01,  2.9573e-01,\n",
            "        -2.4952e-01,  5.1670e-02, -1.6528e-01, -5.4836e-01,  1.1053e-01,\n",
            "         1.8437e-01,  1.3851e-01, -1.0443e-02, -8.8564e-02,  4.7423e-01,\n",
            "        -1.6720e-01, -2.8161e-01, -2.8278e-01,  1.6190e-02,  6.1298e-02,\n",
            "        -2.6301e-01,  3.9287e-01,  2.6594e-01,  1.0703e-01,  1.5469e-01,\n",
            "         1.4666e-01,  1.2119e-01, -1.1021e-01,  2.7049e-01, -2.0445e-01,\n",
            "        -7.2827e-02, -1.2024e-01, -3.0657e-02, -1.3529e-01,  2.7245e-01,\n",
            "        -1.9886e-01,  2.3156e-01, -3.8552e-03, -4.5034e-03, -3.2848e-03,\n",
            "         1.9624e-01,  1.1436e-01,  4.9539e-02,  2.3832e-02, -1.2026e-01,\n",
            "         9.2324e-02,  4.6401e-02, -1.5338e-01,  2.1722e-01,  1.9442e-01,\n",
            "         1.4080e-01, -1.6914e-02,  3.3103e-01, -1.8447e-01,  7.0460e-02,\n",
            "         4.0532e-01, -3.4645e-01, -3.9223e-01, -1.8941e-03, -5.5769e-02,\n",
            "         9.5361e-02, -1.7981e-02, -2.8781e-01,  6.0757e-02,  9.3208e-02,\n",
            "        -1.3353e-01,  4.4488e-02, -3.4692e-01, -3.0394e-01,  1.6845e-01,\n",
            "         1.2389e-01, -2.3326e-01, -2.0068e-01, -4.7088e-01,  6.3973e-02,\n",
            "        -1.1865e-02, -6.8120e-02,  2.4796e-01,  4.0961e-02,  1.1528e-01,\n",
            "        -3.0584e-02, -5.6845e-02, -9.7287e-04,  3.2096e-01,  1.3587e-01,\n",
            "         9.2432e-02,  1.1300e-01,  5.7723e-02,  2.0179e-01, -7.1388e-02,\n",
            "        -1.4847e-01, -2.0185e-01,  5.2732e-01,  1.7858e-01,  1.1876e-02,\n",
            "         6.6864e-02, -5.2967e-02, -1.4562e-01, -8.2319e-01, -4.9354e-01,\n",
            "        -2.5415e-03, -2.1029e-01,  1.5126e-01, -2.7188e-02, -8.0954e-02,\n",
            "         7.0082e-02, -3.5136e-01, -1.3277e-01,  2.4004e-01, -1.2075e-01,\n",
            "        -1.2730e-01, -5.9818e-01,  3.4402e-02,  1.6010e-01,  1.6552e-02,\n",
            "        -1.3956e-01, -1.2046e-01,  3.8689e-02,  4.0405e-01, -3.4025e-01,\n",
            "        -6.9538e-03,  3.8803e-01,  2.4976e-02,  2.2449e-01, -4.5304e-02,\n",
            "         3.3521e-02,  2.7700e-01,  2.7351e-03, -4.5995e-01,  4.6072e-01,\n",
            "        -1.3990e-01,  3.4836e-02, -4.9024e-01,  5.6047e-01, -2.1101e-02,\n",
            "         3.2498e-01,  2.4239e-01, -1.5113e-01, -4.3609e-01,  1.7798e-01,\n",
            "         2.1426e-01, -1.1275e-01, -1.6023e-01, -6.3637e-01,  8.3338e-02,\n",
            "         3.0036e-01,  5.5896e-02, -3.7457e-01,  5.3616e-01,  2.6902e-01,\n",
            "         1.8213e-01, -1.9262e-01, -1.0485e-02, -1.9179e-01, -3.8257e-01,\n",
            "        -3.0331e-01,  2.6794e-01, -6.6121e-02,  2.7979e-01, -3.1598e-01,\n",
            "         2.2178e-01,  1.4917e-01, -2.8839e-01,  7.3577e-02,  3.1169e-01,\n",
            "         2.6827e-01, -3.5052e-01,  3.6715e-01, -3.8260e-01, -5.6303e-01,\n",
            "         3.7226e-02, -3.6328e-02,  1.1285e-01, -2.3378e-01,  2.6612e-02,\n",
            "        -2.3792e-01, -1.3770e-01, -1.6791e-01,  4.4176e-01,  1.3036e-01,\n",
            "        -1.1155e-01, -2.2136e-01,  1.6600e-01, -1.2965e-01, -1.3751e-01,\n",
            "        -5.1510e-03, -1.3615e-01, -1.6438e-01,  2.3666e-01,  1.0372e-02,\n",
            "         3.8355e-01,  3.7581e-01,  2.0372e-01, -4.4086e-01, -2.1512e-01,\n",
            "         1.3921e-01,  6.3353e-02,  8.3098e-02, -3.9427e-01,  2.1980e-01,\n",
            "         3.3776e-02, -1.2120e-01,  6.5413e-01,  1.2904e-01, -2.2145e-01,\n",
            "         4.9193e-01, -2.6376e-01,  4.9829e-01, -2.2790e-01,  4.1216e-01,\n",
            "        -1.9445e-01, -3.6624e-01,  5.6165e-01, -1.9106e-01,  7.7101e-01,\n",
            "         1.9475e-01,  3.1990e-02,  1.9169e-01, -3.3216e-01, -7.6845e-02,\n",
            "         3.8231e-02, -2.0781e-01,  1.5068e-01,  1.0734e-01,  1.8759e-01,\n",
            "        -2.3641e-01,  9.8338e-02,  1.2902e-01, -4.7320e-01,  5.1324e-02,\n",
            "         2.4535e-01,  3.1887e-01, -1.7891e-01,  2.7856e-01,  7.8983e-02,\n",
            "         3.3792e-01,  5.0150e-01, -4.3688e-01, -9.6349e-02, -6.8275e-02,\n",
            "         1.6512e-01,  2.3474e-01,  2.2532e-01, -1.8575e-01, -3.1046e-01,\n",
            "        -1.6289e-01, -1.5756e-01, -6.0471e-03, -1.3764e-01, -7.6813e-02,\n",
            "        -2.0722e-01,  2.5355e-01, -2.8576e-01, -3.1725e-01,  3.1537e-01,\n",
            "         2.9731e-01, -4.0469e-01,  2.3905e-01, -2.8645e-01,  2.1739e-01,\n",
            "         1.9836e-01, -1.6746e-01, -1.9713e-01,  2.9239e-01, -3.6854e-01,\n",
            "         1.6240e-01,  1.0027e-01,  8.6652e-02, -2.6228e-01,  1.0253e-01,\n",
            "         6.2895e-01,  7.9546e-02, -5.9928e-01,  2.9300e-01, -3.1614e-01,\n",
            "        -3.8018e-01, -3.3446e-01,  1.4784e-01, -1.8272e-01,  4.6298e-02,\n",
            "        -7.1736e-02,  3.3898e-01,  1.0273e-01, -1.0685e-01, -4.1400e-01,\n",
            "         1.8753e-02, -2.1366e-01, -9.9958e-02,  9.4127e-02, -9.9425e-02,\n",
            "         7.6413e-03, -8.1060e-02,  4.9449e-02, -2.5396e-01,  1.3088e-01,\n",
            "        -6.0705e-02, -3.3184e-01,  1.1097e-01, -4.5878e-01,  3.9175e-01,\n",
            "         1.3352e-01,  1.2199e-02,  5.3805e-02,  1.7289e-01,  1.4570e-01,\n",
            "         3.4887e-01,  1.5726e-02, -1.3667e-01,  1.9232e-02, -7.9142e-02,\n",
            "         1.7404e-01,  1.0745e-01, -1.8977e-01,  1.0837e-02, -4.4289e-02,\n",
            "         3.1643e-02, -1.3593e-01,  1.4071e-01,  2.4817e-01,  1.5338e-01,\n",
            "        -9.6664e-02, -3.1337e-01,  1.0756e-01,  1.7378e-01, -8.1325e-02,\n",
            "        -2.7816e-01, -2.1186e-01, -9.2081e-02, -3.5559e-01, -1.5779e-01,\n",
            "        -3.1640e-02, -2.7001e-01, -8.8447e-02,  2.0570e-01, -7.8722e-03,\n",
            "         2.6537e-01, -5.1532e-01,  1.3778e-01, -3.1079e-01, -2.1826e-01,\n",
            "        -2.1808e-01, -2.1302e-01, -9.1567e-02,  1.7409e-02,  9.3281e-02,\n",
            "         1.1568e-01,  4.3557e-01,  4.4121e-01,  1.6173e-02,  8.5567e-02,\n",
            "        -2.3086e-02,  2.1964e-01,  3.3389e-01, -3.1838e-01, -2.0247e-01,\n",
            "         8.6908e-02, -1.1952e-01,  2.6714e-01, -9.6712e-03,  3.8355e-02,\n",
            "        -6.7285e-02,  7.0458e-02, -4.0162e-01,  1.7021e-01,  1.0719e-01,\n",
            "        -1.8463e-01, -2.4593e-01,  1.2828e-01, -7.8668e-02,  5.9277e-02,\n",
            "         1.2514e-01, -5.2141e-01,  1.9521e-01,  2.9680e-01,  2.0141e-01,\n",
            "        -4.2439e-01,  7.9615e-02, -1.0399e-01,  2.4307e-01,  1.9270e-01,\n",
            "         1.7350e-01, -2.1626e-02,  8.5483e-02,  1.3283e-01,  7.5995e-02,\n",
            "        -1.4896e-01,  5.6485e-02, -6.5670e-02, -1.3539e-01, -8.6110e-02,\n",
            "         1.1658e-01, -1.4273e-01, -3.4514e-01,  3.7246e-01,  9.7439e-02,\n",
            "        -7.4148e-02, -2.1660e-01, -2.5929e-01, -2.5186e-02,  6.0736e-02,\n",
            "         1.3463e-01, -3.0705e-01, -1.4335e-01, -1.5739e-01,  2.7412e-01,\n",
            "        -6.5734e-02,  8.6861e-02, -6.6296e-03, -1.3186e-01,  1.3960e-01,\n",
            "         4.9491e-02, -1.6456e-01, -5.5161e-02,  6.5277e-02,  1.6937e-01,\n",
            "        -5.1939e-01,  1.2435e-01,  1.4691e-01,  2.3012e-01,  1.7388e-01,\n",
            "        -2.8309e-01,  3.2325e-01, -1.3710e-01,  1.2524e-01,  2.4304e-01,\n",
            "        -2.6327e-01, -1.0671e-01,  7.8577e-01, -7.6533e-02,  2.5800e-01,\n",
            "         2.4751e-01,  3.1426e-01,  1.7870e-01, -2.8140e-02, -1.6852e-01,\n",
            "         1.6901e-01,  1.9875e-01,  2.4257e-01, -9.4173e-03,  1.5071e-01,\n",
            "        -2.8919e-01,  1.5249e-01, -1.7954e-01,  8.8237e-02,  2.5362e-01,\n",
            "         3.3620e-01,  4.1926e-01, -8.8220e-02, -2.1273e-01,  2.1360e-02,\n",
            "         2.4710e-01, -1.8368e-02,  2.3283e-02,  5.4182e-02, -1.5587e-01,\n",
            "         3.7798e-02,  5.1332e-02, -3.9067e-01,  8.3042e-02, -1.1673e-01,\n",
            "        -1.1898e-01, -2.2272e-01,  3.6927e-02,  1.6912e-01, -9.6647e-02,\n",
            "         5.5249e-01, -1.3801e-01, -4.0355e-01, -1.2220e-02,  5.3098e-02,\n",
            "        -5.0987e-01, -8.0816e-01,  1.2093e-01, -5.2574e-02, -2.7690e-01,\n",
            "         8.8735e-02, -2.4193e-01,  9.7758e-02,  1.0393e-01,  1.3998e-01,\n",
            "         2.9537e-01, -1.8768e-01, -1.1803e-01, -2.4168e-01,  4.6989e-02,\n",
            "        -2.2445e-01,  5.7077e-01,  1.5087e-02,  1.0503e-01, -3.1319e-02,\n",
            "         6.9600e-02, -9.0017e-02,  2.7728e-01, -8.8998e-02,  2.2287e-01,\n",
            "        -9.1567e-02, -2.5188e-01, -2.0373e-01, -2.4184e-01,  2.0198e-01,\n",
            "         4.1378e-02,  4.3308e-01,  5.5232e-02, -2.4005e-01,  3.6826e-01,\n",
            "         5.3675e-01, -1.2580e-01,  1.1280e-01,  3.8933e-01,  1.6416e-03,\n",
            "        -1.4203e-01,  4.1253e-01, -1.0737e-01, -1.6547e-01, -3.7192e-02,\n",
            "         5.5512e-01, -5.5979e-03, -7.9009e+00,  2.1368e-01, -1.7928e-01,\n",
            "        -5.9418e-02,  2.2638e-01, -7.0066e-02,  1.9867e-01,  1.7893e-01,\n",
            "        -1.8014e-02, -6.2793e-02,  1.1404e-01, -1.5634e-01,  1.3372e-01,\n",
            "         3.1881e-01,  2.2712e-01, -4.3367e-01,  3.4627e-01,  2.2693e-01,\n",
            "         5.9085e-01, -1.5335e-01, -1.0440e-01, -1.4621e-01,  7.1004e-02,\n",
            "         5.2894e-02,  2.9271e-01, -6.9423e-02,  1.8441e-01, -6.5995e-02,\n",
            "         4.4364e-01,  1.7969e-01, -2.9549e-01,  3.1627e-01, -3.5783e-02,\n",
            "        -3.0866e-01, -3.3508e-02,  2.0910e-01,  3.1557e-01, -1.3080e-01,\n",
            "         3.1316e-02, -5.2574e-01, -4.2929e-02,  5.2158e-02, -6.1241e-03,\n",
            "        -1.3298e-01,  2.5302e-01, -1.6869e-01, -1.9518e-02,  1.0013e-03,\n",
            "        -4.2107e-02, -1.5382e-01,  3.6888e-01,  2.9968e-01,  9.1908e-02,\n",
            "        -3.6984e-01, -1.9091e-02,  5.3149e-02, -5.1945e-01,  3.9984e-01,\n",
            "         2.9662e-01,  6.9221e-02, -9.9624e-03, -3.3255e-02,  9.6482e-03,\n",
            "        -3.4931e-01, -8.3232e-02,  7.5828e-02, -3.2718e-01,  8.5375e-02,\n",
            "         9.6754e-02, -3.7934e-01, -4.1073e-01,  6.4564e-04, -9.6015e-02,\n",
            "        -2.5233e-01, -1.5689e-01, -2.1814e-01, -6.0836e-02,  4.2984e-02,\n",
            "         2.7151e-01,  1.5197e-01, -1.4185e-01, -7.9469e-02, -3.8324e-01,\n",
            "        -1.7140e-01, -1.1470e-01, -1.4135e-01, -3.9993e-01, -3.1042e-02,\n",
            "        -5.0340e-01, -6.4843e-02,  3.1633e-01, -1.7410e-01,  1.5043e-01,\n",
            "         2.4733e-01, -3.5868e-01,  1.1562e-01, -1.2798e-01, -3.4758e-01,\n",
            "        -9.5296e-02,  4.9847e-02, -1.3976e-01, -2.5480e-02, -3.3142e-02,\n",
            "         9.7614e-02,  2.4802e-01,  3.3990e-01,  2.4072e-02, -5.3582e-01,\n",
            "        -6.3072e-02,  9.7434e-02,  3.7115e-01,  6.0164e-01, -1.9747e-01,\n",
            "         1.1453e-01,  1.7993e-01,  4.7801e-01,  3.1747e-01, -7.1746e-02,\n",
            "        -3.0914e-02, -4.0231e-01,  3.2018e-02, -2.0468e-02,  5.1846e-01,\n",
            "        -8.0113e-02,  1.5567e-01,  3.0817e-01,  5.4830e-01,  7.5530e-02,\n",
            "         7.4676e-02, -7.6326e-02,  2.9158e-01,  5.5402e-01, -4.0575e-01,\n",
            "         4.6489e-01, -1.1677e-01, -1.7126e-01,  1.3401e-01, -1.9296e-01,\n",
            "        -3.4444e-01,  2.3245e-01, -5.0130e-01, -3.5834e-01,  2.1404e-01,\n",
            "         2.8171e-01,  3.7295e-01,  1.1626e-01,  8.9709e-02,  1.7283e-01,\n",
            "         7.8259e-02, -4.1044e-01, -1.2625e-01,  4.6419e-01,  1.2349e-01,\n",
            "        -3.6851e-01, -4.7941e-01, -1.5404e-01, -7.7226e-02,  2.5527e-01,\n",
            "        -1.6853e-01, -1.1724e-01, -5.0285e-02,  4.1464e-01,  1.0483e-01,\n",
            "         5.0534e-02,  1.4108e-01, -2.1294e-01,  1.5359e-01, -1.5143e-02,\n",
            "        -1.0748e-01, -1.7877e-01, -1.0514e-01,  1.6952e-02,  1.9744e-01,\n",
            "         9.1620e-02, -1.5705e-01, -3.0934e-01,  1.6035e-01, -4.1276e-02,\n",
            "        -3.6079e-01,  3.1110e-01, -7.8875e-02,  4.2299e-01,  3.4321e-01,\n",
            "        -2.9969e-01, -1.9867e-01,  4.0886e-01,  1.0143e-01,  3.6624e-01,\n",
            "         7.4180e-02, -1.9090e-01, -1.2714e-01, -3.2096e-01, -3.2822e-01,\n",
            "         2.1389e-01, -2.6752e-02,  2.6553e-01, -4.1649e-01,  4.2521e-02,\n",
            "        -1.1791e-01,  3.8554e-01, -7.5115e-02,  1.3972e-01,  1.1739e-01,\n",
            "        -5.3172e-02, -1.8361e-01, -1.4695e-01, -3.9158e-01,  9.5828e-02,\n",
            "         2.7648e-01, -1.7646e-01,  9.8800e-02])\n"
          ]
        }
      ]
    }
  ]
}